# ğŸµ Azure Data Factory â€” Spotify Incremental Ingestion Pipeline

This repository contains a complete **Azure Data Factory (ADF)** project that implements an automated, **metadata-driven incremental ingestion pipeline** for Spotify analytics data.

The pipeline extracts data from **Azure SQL Database**, loads **only new or updated records**, supports backfilling, and stores the output into **Azure Data Lake Storage Gen2 (ADLS)** in **Parquet** format.

> **Target Audience:** Analytics Engineers, BI Engineers, and Data Engineers looking for real-world data engineering patterns.

---

## ğŸ“ Repository Structure

```text
azure-data-factory-spotify-project/
â”‚
â”œâ”€â”€ datasets/source_scripts
â”‚   â”œâ”€â”€ loop_input.txt
â”‚   â”œâ”€â”€ new_updated_date.json
â”‚   â”œâ”€â”€ spotify_incremental_load.sql
â”‚   â”œâ”€â”€ spotify_initial_load.sql
â”‚   â””â”€â”€ last_updated_date.json
â”‚
â”œâ”€â”€ assets/
â”‚   â”œâ”€â”€ 01_ds_spotify_source_sqldb.png
â”‚   â”œâ”€â”€ 02_ds_spotify_sink_bronze_adls.png
â”‚   â”œâ”€â”€ â€¦ (16 total images)
â”‚
â””â”€â”€ README.md   â† You are here
````

-----

## ğŸš€ Project Overview

This project demonstrates how to build a **production-grade incremental data pipeline** using Azure Data Factory.

### âœ… Incremental Ingestion Logic

  * Reads source tables from **Azure SQL DB**.
  * Extracts only records where last-updated-date > previously-processed-date.
  * Writes the new/updated data to **ADLS (Bronze Layer)**.

### âœ… Backfilling Support

  * If historical data is corrected or changed, the pipeline can reprocess past records by simply adjusting the **JSON metadata file**.

### âœ… Dynamic Metadata-Driven Pipeline

  * A **single pipeline loops** through multiple tables dynamically.
  * Table details (`schema`, `table name`, `output folder`, `date column`) are passed as items into a **ForEach** loop.

### âœ… Automated File Tracking (JSON)

  * `last_updated_date.json` stores the **last ingestion timestamp**.
  * The pipeline updates this file automatically after every successful run.

### âœ… Safe Data Handling

  * If **no incremental data is found**:
      * The empty landing file is deleted.
      * Metadata is **NOT** updated, preventing data drift.

-----

## ğŸ—ï¸ Solution Architecture

### ğŸ”¹ Source System

  * **Azure SQL Database**: Tables contain a `last_updated_date` column used for incremental detection.

### ğŸ”¹ Pipeline Components

  * Linked Services, Datasets, Lookup, Copy Activity, Script Activity, Set Variable, If Condition, Delete Activity.

### ğŸ”¹ Sink Storage

  * **Azure Data Lake Gen2**:
      * **Path Structure:** `/spotify/bronze/{table_name}/{file_processed_date}/`
      * **Format:** Parquet (Snappy Compression).

-----

## ğŸ“¦ Key Pipeline Components

### 1ï¸âƒ£ Source Dataset â€” Azure SQL Database

**ğŸ“· Screenshot:** ![01\_ds\_spotify\_source\_sqldb.png](./assets/01_ds_spotify_source_sqldb.png)

A dynamic SQL source dataset pointing to Azure SQL DB.

  * **Schema & Table:** Derived from ForEach items.
  * **Query Logic:**
    ```sql
    SELECT * FROM @{item().schema}.@{item().table}
    WHERE @{item().last_updated_date} > @{activity('File Last Updated Date').output.value[0].last_updated_date}
    ```

### 2ï¸âƒ£ Bronze Sink â€” ADLS Parquet Dataset

**ğŸ“· Screenshot:** ![02\_ds\_spotify\_sink\_bronze\_adls.png](./assets/02_ds_spotify_sink_bronze_adls.png)

Dynamic file path construction:

```text
/spotify/bronze/@{dataset().folder_name}/@{dataset().file_processed_date}/
```

  * `folder_name` â†’ derived from metadata.
  * `file_processed_date` â†’ current timestamp via pipeline variable.

### 3ï¸âƒ£ JSON Tracking Files

**ğŸ“· Screenshots:**

  * ![03\_ds\_spotify\_new\_updated\_date\_json.png](./assets/03_ds_spotify_new_updated_date_json.png)

  * ![04\_ds\_spotify\_last\_updated\_date\_json.png](./assets/04_ds_spotify_last_updated_date_json.png)

  * **`last_updated_date.json`**: Stores the last processed timestamp for each table.

  * **`new_updated_date.json`**: Temporary file used during metadata update operations.

### 4ï¸âƒ£ ForEach Loop â€” Metadata Driven

**ğŸ“· Screenshots:**

  * ![05\_for\_each\_1.png](./assets/05_for_each_1.png)
  * ![05\_for\_each\_2.png](./assets/05_for_each_2.png)

The pipeline iterates through a list of tables defined in the pipeline parameters:

```json
[
  {
    "schema": "dbo",
    "table": "spotify_tracks",
    "output_folder_name": "spotify_tracks",
    "last_updated_date": "updated_at"
  }
]
```

### 5ï¸âƒ£ Lookup & Variable Setting

**ğŸ“· Screenshots:**

  * ![06\_if else main.png](./assets/06_if_else_main.png)

  * ![10\_file processed date.png](./assets/10_file_processed_date.png)

  * **Lookup:** Reads existing `last_updated_date.json` to determine the waterline.

  * **Set Variable:** Captures `@utcNow()` to use for the file processed date folder.

### 6ï¸âƒ£ Copy Activity â€” SQL DB â†’ ADLS Bronze

**ğŸ“· Screenshots:**

  * ![11\_copy data source.png](./assets/11_copy_data_source.png)

  * ![12\_cop data sink.png](./assets/12_copy_data_sink.png)

  * Source query is dynamically built using the Table Name, Schema, and Date logic.

  * Sink writes **Parquet** files using **Snappy** compression.

### 7ï¸âƒ£ IF Condition â€” Check Incremental Data

**ğŸ“· Screenshot:** ![06\_if else main.png](./assets/06_if_else_main.png)

**Logic:** `@greater(activity('Copy from SQLDB to ADLS').output.dataRead, 0)`

| Result | Action | Screenshots |
| :--- | :--- | :--- |
| **TRUE** (New Data) | 1. Calculate Max Date (Script)<br>2. Update `last_updated_date.json` | ![07\_if else true.png](./assets/07_if_else_true.png)<br>![13\_max update date.png](./assets/13_max_update_date.png)<br>![14\_update the last updated date source.png](./assets/14_update_the_last_updated_date_source.png)<br>![15\_update the last updated date sink.png](./assets/15_update_the_last_updated_date_sink.png) |
| **FALSE** (No Data) | 1. Delete empty file | ![08\_if else false.png](./assets/08_if_else_false.png)<br>![16\_delete.png](./assets/16_delete.png) |

-----

## ğŸ” Backfilling Support

To force historical reprocessing, manually edit the metadata JSON in ADLS:

```json
{
  "last_updated_date": "2020-01-01T00:00:00Z"
}
```

**Use Cases:** Data corrections, late-arriving changes, or full replay scenarios.

-----

## ğŸ“Š Final Output (Bronze Layer)

Files are generated in the following structure:

> `/spotify/bronze/{table_name}/{file_processed_date}/part-*.snappy.parquet`

**Benefits:**

  * **Dynamic folder structure** for easy partitioning.
  * **Efficient columnar storage** (Parquet).
  * **Ready for downstream** Silver/Gold transformation pipelines.
